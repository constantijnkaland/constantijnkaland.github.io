# 0: general information ####
# This script performs hierarchical cluster analysis on F0 contours measured with "time-series_F0.praat". A graphical user interface for this script is provided as 'contour_clustering_gui.R'.
# A theoretical motivation is given in the accompanying paper. 
# Usage guidelines are given in the accompanying manual, referred to in this script by comments after "## Manual ..."
# Additional in-line comments are given after single "#"
# Code-block headings are commented by final "####" to indicate the script's structure in R studio
# Developed and tested using package versions:
# dplyr_1.0.7
# reshape2_1.4.4
# reshape_0.8.8
# ggplot2_3.3.5
# shiny_1.6.0 
#
# Constantijn Kaland, August 2021.
# https://constantijnkaland.github.io/contourclustering/
#
# 1: set variables ####
# in this code block the most important settings for data handling and cluster analysis can be configured
# variables set here have important implications throughout the script
# this codeblock does not provide variables for subsetting; check subsetting code blocks individually for this

# datafile with F0 measurements as obtained by the Praat script, include entire path ["abc123.csv"]
# note: working directory is automatically set to the directory where this script and the datafile with F0 measurements are stored ["AUTO"]
# working directory can be specified manually ["P:/ath/to/working/dir/"]
## Manual 2.2.
workdir = "AUTO"
datafile = "pmy_scripted_noun_phrases.csv"

# number of measurements per contour (as set in the Praat script) 
# use "AUTO" for automatic detection based on number of columns in the datafile (change to manual value in case of errors) [a number] / ["AUTO"]
## Manual 2.2.1.
nmsr = "AUTO"

# set percentage of allowed change due to octave jump killing [0-100]
## Manual 2.2.1.
killperc = 10

# set number of clusters for main analysis [a number]
## Manual 2.2.3.
numbclust = 8

# N.B. set variable for subsetting in the respective sections below

# 2: load and install required packages ####
# try manual installation if this code-block gives errors

if (workdir == "AUTO") {
  if (!require("rstudioapi")) install.packages("rstudioapi")
  library(rstudioapi)
  current_path = rstudioapi::getActiveDocumentContext()$path
  setwd(dirname(current_path))
} else {
  setwd(workdir)
}

if (!require("ggplot2")) install.packages("ggplot2")
library(ggplot2)

if (!require("reshape")) install.packages("reshape")
library(reshape)
if (!require("reshape2")) install.packages("reshape2")
library(reshape2)

if (!require("dplyr")) install.packages("dplyr")
library(dplyr)


# 3: read datafile ####

# note that reading the datafile in this way assumes the output file directly generated by the Praat script:
# the Praat script output file is a comma-separated csv, with the column headings matching the variable names in this script
# any datafile generated differently or altered manually might require different variable names and/or different ways of reading
# consider using additional arguments such as 'stringsAsFactors = T/F', 'skipNul = T' or 'fileEncoding = "latin1"' for ortographically challenging textgrids.
## Manual 2.2.1.
as.data.frame(read.csv(datafile,sep=",", header = T,row.names = NULL,stringsAsFactors = T)) -> data


# 4: select and prepare data for analysis ####
## Manual 2.2.2.

#remove empty filenames and segment labels
subset(data, data$filename !="")->data
subset(data, data$interval_label !="")->data
#remove unused levels 
droplevels(data) -> data
#remove F0 measurement errors
subset(data, !grepl("--undefined--", data$f0))->data
subset(data, data$f0 !="")->data
# remove contours where octave jump killing changed mean F0 of contour with more than XX%
killprop = killperc/100
subset(data, between(as.numeric(as.character(data$jumpkilleffect)),left = (1-killprop),right = (1+killprop))) -> data
# set Y-label for plots to Hertz
"(Hz)" ->> ysc
paste("f0 ",ysc,sep="") ->> ylb
# read number of F0 measurements either automatically or manually depending on variable setting 'nmsr'
if (nmsr == "AUTO") {
  length(levels(as.factor(data$stepnumber))) -> steps
} else {
  nmsr -> steps
}
# replace :; with commas (Praat script uses :; for every comma in the textgrid annotations, to ensure comma-separated file reading compatibility with this script)
gsub(pattern = ":;",replacement = ",",x = as.character(data$interval_label)) -> data$interval_label
# convert to semitones and change Y-label in plots accordingly
log10((as.numeric(as.character(data$f0))/50))*39.87 -> data$f0
"(ST)" ->> ysc
paste("f0 ",ysc,sep="") ->> ylb

# 5: control for speaker differences ####
## Manual 2.2.3.

# 1: subtract mean
# f0(spk)-mean.f0(spk)
paste("Speaker mean corrected f0 ",ysc,sep="") ->> ylb
for (spk in levels(as.factor(data$filename))) {
  mean(as.numeric(as.character(data$f0[data$filename==spk]))) -> m
  (as.numeric(as.character(data$f0[data$filename==spk]))-m) -> data$f0[data$filename==spk]
}


# 2: standardise
# f0(spk)-mean.f0(spk) / sd.f0(spk)
"Speaker standardized f0" ->> ylb
for (spk in levels(as.factor(data$filename))) {
  mean(as.numeric(as.character(data$f0[data$filename==spk]))) -> m
  sd(as.numeric(as.character(data$f0[data$filename==spk]))) -> sd
  (as.numeric(as.character(data$f0[data$filename==spk]))-m)/sd -> data$f0[data$filename==spk]
}

# 3: min-max normalize
# f0(spk)-min.f0(spk) / max.f0(spk)-min.f0(spk)
"Speaker min-max normalized f0" ->> ylb
for (spk in levels(as.factor(data$filename))) {
  mean(as.numeric(as.character(data$f0[data$filename==spk]))) -> m
  min(as.numeric(as.character(data$f0[data$filename==spk]))) -> min
  max(as.numeric(as.character(data$f0[data$filename==spk]))) -> max
  (as.numeric(as.character(data$f0[data$filename==spk]))-min)/(max-min) -> data$f0[data$filename==spk]
}

# 4: robust scaling
# f0(spk)-median.f0(spk) / 75q.f0-25q.f0
"Speaker robust scaled f0" ->> ylb
for (spk in levels(as.factor(data$filename))) {
  median(as.numeric(as.character(data$f0[data$filename==spk]))) -> md
  quantile(as.numeric(as.character(data$f0[data$filename==spk])), 0.75) -> q75
  quantile(as.numeric(as.character(data$f0[data$filename==spk])), 0.25) -> q25
  (as.numeric(as.character(data$f0[data$filename==spk]))-md)/(q75-q25) -> data$f0[data$filename==spk]
}

# 5: Octave-Median scaling
# log2(f0(spk)/median.f0(spk))
"Octave-Median scaled f0" ->> ylb
for (spk in levels(as.factor(data$filename))) {
  median(as.numeric(as.character(data$f0[data$filename==spk]))) -> md
  log2(as.numeric(as.character(data$f0[data$filename==spk]))/md) -> data$f0[data$filename==spk]
}

# 6: prepare data for cluster analysis / subsetting ####

# make dataset "wide" for cluster analysis
dcast(data,filename+interval_label+start+end~stepnumber,value.var="f0") -> datacast

# read which colum name is called "1" to identify the measures
which( colnames(datacast)=="1" ) -> stepone

# optional: automatic subsetting ####
## Manual 2.2.6.

# subset the data by removing the smallest cluster after each cluster analysis until a specified proportion of the data left.
# subsetting gets more precise with higher numbers of assumed clusters. 
# subsetting can be run before or after main cluster analysis (as found in the next codeblock)

# set number of clusters for automatic subsetting, should be more than the specified number of clusters for main analysis [a number]
# default value is twice the specified number of clusters for the main analysis, which is probably a sufficient approximation (change only if needed)
ssclust = numbclust*2

# set proportion of data remaining after automatic subsetting (use 1 for no subsetting) [0-1]
# if value is set to 1, no automatic subsetting is applied and manual subsetting will be applied
prop = 1

# read number of rows
nrow(datacast) -> allrows
# subsetting (automatically runs one cluster analysis if there is no colum with the name 'cluster')
while (nrow(datacast)>allrows*prop)
{
  if ("cluster" %in% colnames(datacast)) {
    match(min(table(datacast$cluster)), table(datacast$cluster)) -> min
    subset(datacast,datacast$cluster!=min) -> datacast
    print(table(datacast$cluster))
    print(sum(table(datacast$cluster)))
    dist_mat <- dist(datacast[,stepone:((stepone-1)+steps)], method = 'euclidean')
    hclust_avg <- hclust(dist_mat, method = 'complete')
    cut_avg <- cutree(hclust_avg, k = ssclust)
    datacast <- mutate(datacast, cluster = cut_avg)
  } else {
    dist_mat <- dist(datacast[,stepone:((stepone-1)+steps)], method = 'euclidean')
    hclust_avg <- hclust(dist_mat, method = 'complete')
    cut_avg <- cutree(hclust_avg, k = ssclust)
    datacast <- mutate(datacast, cluster = cut_avg)
  }
}


# optional: manual subsetting ####
## Manual 2.2.6.

# subsetting the data by removing specified clusters
# can only be run if at least one cluster analysis has been done (and cluster values have been written to data)
# inspection of the main cluster analysis is needed before using this subsetting procedure

# re-set variable after each analysis!
# set the variable indicating which cluster(s) should be removed [c(1,2,3)]
# if set to c(), no manual subsetting is carried out (use this option if automatic subsetting is needed)
rem_clust = c(1,2,3)

# remove each cluster indicated for removal and update data
# cluster analysis needs to be run again after each manual subsetting round

for (clust in rem_clust) {
  subset(datacast, datacast$cluster!=clust) -> datacast
}




# 7: cluster analysis ####
## Manual 2.2.4.


# create a distance matrix based on the euclidean distances
dist_mat <- dist(datacast[,stepone:((stepone-1)+steps)], method = 'euclidean')
# create a dendrogram and plot it with a red border around each cluster
hclust_avg <- hclust(dist_mat, method = 'complete')
plot(hclust_avg)
rect.hclust(hclust_avg, k = numbclust, border=2)
# create an output with the specified number of clusters
cut_avg <- cutree(hclust_avg, k = numbclust)

# write the cluster number back to the data 
datacast <- mutate(datacast, cluster = cut_avg)

###########################################################################################
for (i in 1:8){
  mean(datacast$1[datacast$cluster==1,])
}


# create a table with the cluster sizes (N) and
# mean standard errors over all measurement points per cluster (M se)
sevt = c()
for (c in 1:numbclust) {
  sev = c()
  for (stp in 0:(steps-1)) {
  sd(subset(datacast[stp+stepone], datacast$cluster==c)[,1])/
    sqrt(nrow(subset(datacast, datacast$cluster==c))) -> se
  append(sev,se) -> sev
  }
  append(sevt,mean(sev)) -> sevt
}
rbind(table(datacast$cluster),sevt) -> table
round(table,2) -> table
rownames(table) <- c("N","M se")


# check the table for suspicious clusters (either N=1 or M se > 2*median)
flagged = c()
for (c in (1:numbclust)) {
  append(flagged, ifelse(table[1,c]==1|table[2,c]>2*median(sevt, na.rm = T),yes = 1,no = 0)) -> flagged
}
rbind(table,flagged) -> table

# mark flagged clusters for removal (this makes the manual subsetting semi-automatic)
## Manual 2.2.4.
rem_clust = c()
for (c in (1:numbclust)) {
  ifelse(table[3,c]!=1,"",append(rem_clust,c)->rem_clust)
}

# show table
table
  
    


# 8: plot the mean contour per cluster ####
## Manual 2.2.5.

# make dataset "long" for plotting
melt(datacast,id.vars = c("filename","interval_label","start", "end","cluster")) -> dataplot
as.numeric(dataplot$variable) -> dataplot$variable

# 4 equidistant x-axis markers are set automatically for later use in the plot, depending on the number of measures (steps)
brks = c((steps*0.25),(steps*0.5),(steps*(0.75)),steps)

# prepare panel labels
panel_text <- data.frame(cluster = 1:numbclust, label = paste("n = ", as.character(as.data.frame(table(datacast$cluster))[,2]),sep=""))

#plot the mean values (and standard error in shaded area) for each measurement point for each cluster, 
#the number of observations are plotted for each cluster

ggplot(dataplot, aes(x=variable, y = value)) +
  stat_summary(fun=mean, group="cluster", geom="line", show.legend = F) +
  stat_summary(fun.data = mean_sdl, group="cluster", geom = "ribbon", alpha = .2,show.legend = F) +
  scale_x_continuous(breaks=brks) + 
  facet_wrap(~cluster,ncol = 4) +
  xlab("measurement number") +
  ylab(ylb) +
  theme(axis.title = element_text(size = 20),axis.text = element_text(size=20),strip.text = element_text(size = 20)) +
  geom_text(data = panel_text, mapping = aes(x = 0.5*stepsN, y = 0.95*max(dataplot$value), label = label)) -> plot


# show plot
plot


# 9: annotate clusters into a TextGrid for each speaker ####
## Manual 2.2.5.

# this codeblock generates textgrids that can be merged with the original textgrid used to obtain F0 measurements.
# generated textgrids are stored in the working directory

for (spk in levels(as.factor(data$filename))) {
  sink(paste(spk,".TextGrid",sep = ""))
  cat("File type = \"ooTextFile\"")
  cat("\n")
  cat("Object class = \"TextGrid\"")
  cat("\n")
  cat("\n")
  cat("xmin = 0")
  cat("\n")
  xmax <- max(datacast$start[datacast$filename==spk])+.5
  cat(paste("xmax = ",xmax,sep = ""))
  cat("\n")
  cat("tiers? <exists>")
  cat("\n")
  cat("size = 1")
  cat("\n")
  cat("item []:")
  cat("\n")
  cat("    item [1]:")
  cat("\n")
  cat("        class = \"IntervalTier\"")
  cat("\n")
  cat("        name = \"cluster\"")
  cat("\n")
  cat("        xmin = 0")
  cat("\n")
  cat(paste("        xmax = ",xmax, sep = ""))
  cat("\n")
  cat(paste("        intervals: size = ",nrow(sort_df(datacast[datacast$filename==spk,],"start")),sep = ""))
  cat("\n")
  for (contour in 1:nrow(sort_df(datacast[datacast$filename==spk,],"start"))) {
    cat(paste("        intervals [",contour,"]:", sep = ""))
    cat("\n")
    cat(paste("            xmin = ",subset(sort_df(datacast, "start"),filename==spk)[contour,"start"],sep = ""))
    cat("\n")
    cat(paste("            xmax = ",subset(sort_df(datacast, "start"),filename==spk)[contour,"end"],sep = ""))
    cat("\n")
    cat(paste("            text = \"",subset(sort_df(datacast, "start"),filename==spk)[contour,"cluster"],"\"",sep = ""))
    cat("\n")
  }
  sink()
}
